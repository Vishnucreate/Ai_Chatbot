{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOKHwAQ2GOzs",
        "outputId": "e0177bda-5f6e-44ff-aed5-6856e421d58f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ScFJPQEkFzj_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/500Kwords.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)\n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "cXpaCyBkGxPe",
        "outputId": "7223bd4d-48f3-45af-c5b6-444d9aff3cf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This page intentionally left blank crazy-readers.blogspot.com Artificial Intelligence A Modern Approach Third Edition crazy-readers.blogspot.com PRENTICE HALL SERIES IN ARTIFICIAL INTELLIGENCE Stuart Russell and Peter Norvig, Editors F ORSYTH & P ONCE G RAHAM J URAFSKY & M ARTIN N EAPOLITAN RUSSELL & N ORVIG Computer Vision: A Modern Approach ANSI Common Lisp Speech and Language Processing, 2nd ed. Learning Bayesian Networks Artificial Intelligence: A Modern Approach, 3rd ed. Artificial Intellig'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A-gSM-zG4MY",
        "outputId": "2aa3a395-fcfa-428d-facc-c74b42b8d168"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "502009"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7nzPX01G8Br",
        "outputId": "ee50048d-40fb-4b60-fa36-4495acebcaff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[19, 473, 2959, 238, 865, 2960, 1321, 2961, 2962, 169, 144, 3, 631, 252, 1010]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wclETYqGHBpQ",
        "outputId": "ad7b5563-fa94-4b01-c14c-635802cfef9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83681"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI4jF86VHSby",
        "outputId": "06f08ce4-2304-49c7-bea8-bccbecd41950"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7782\n",
            "The Length of sequences are:  83678\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  19,  473, 2959,  238],\n",
              "       [ 473, 2959,  238,  865],\n",
              "       [2959,  238,  865, 2960],\n",
              "       [ 238,  865, 2960, 1321],\n",
              "       [ 865, 2960, 1321, 2961],\n",
              "       [2960, 1321, 2961, 2962],\n",
              "       [1321, 2961, 2962,  169],\n",
              "       [2961, 2962,  169,  144],\n",
              "       [2962,  169,  144,    3],\n",
              "       [ 169,  144,    3,  631]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "UnLhWVC1HUwH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wWvQS09HYF1",
        "outputId": "5fd95199-61c7-486c-ab73-0a7df8159381"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[  19  473 2959]\n",
            " [ 473 2959  238]\n",
            " [2959  238  865]\n",
            " [ 238  865 2960]\n",
            " [ 865 2960 1321]\n",
            " [2960 1321 2961]\n",
            " [1321 2961 2962]\n",
            " [2961 2962  169]\n",
            " [2962  169  144]\n",
            " [ 169  144    3]]\n",
            "Response:  [ 238  865 2960 1321 2961 2962  169  144    3  631]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAXcrNwSHcTb",
        "outputId": "962db98e-464a-4d11-f8c1-77ceedec89ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "b74Xgg8EHfgN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnWApUuI5jZ",
        "outputId": "19064b80-5c37-4f20-bd93-385ec66c2635"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             77820     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7782)              7789782   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20916602 (79.79 MB)\n",
            "Trainable params: 20916602 (79.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assuming 'model' is your Keras model\n",
        "plot_model(model, to_file='plot.png', show_layer_names=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHnUmAf4JKo2",
        "outputId": "47add478-9858-4f2a-818e-8269128bb55d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL6m5npbJSDC",
        "outputId": "5c68c9b8-a2ac-4fc2-f620-519772fda678"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 6.8214\n",
            "Epoch 1: loss improved from inf to 6.82142, saving model to next_words.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1308/1308 [==============================] - 120s 89ms/step - loss: 6.8214\n",
            "Epoch 2/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 6.2539\n",
            "Epoch 2: loss improved from 6.82142 to 6.25389, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 6.2539\n",
            "Epoch 3/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 5.8545\n",
            "Epoch 3: loss improved from 6.25389 to 5.85447, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 5.8545\n",
            "Epoch 4/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 5.5262\n",
            "Epoch 4: loss improved from 5.85447 to 5.52617, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 5.5262\n",
            "Epoch 5/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 5.2320\n",
            "Epoch 5: loss improved from 5.52617 to 5.23201, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 5.2320\n",
            "Epoch 6/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 4.9467\n",
            "Epoch 6: loss improved from 5.23201 to 4.94675, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 4.9467\n",
            "Epoch 7/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 4.6621\n",
            "Epoch 7: loss improved from 4.94675 to 4.66209, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 119s 91ms/step - loss: 4.6621\n",
            "Epoch 8/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 4.3673\n",
            "Epoch 8: loss improved from 4.66209 to 4.36731, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 4.3673\n",
            "Epoch 9/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 4.0667\n",
            "Epoch 9: loss improved from 4.36731 to 4.06668, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 4.0667\n",
            "Epoch 10/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 3.7660\n",
            "Epoch 10: loss improved from 4.06668 to 3.76605, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 3.7660\n",
            "Epoch 11/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 3.4723\n",
            "Epoch 11: loss improved from 3.76605 to 3.47232, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 120s 91ms/step - loss: 3.4723\n",
            "Epoch 12/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 3.1899\n",
            "Epoch 12: loss improved from 3.47232 to 3.18988, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 119s 91ms/step - loss: 3.1899\n",
            "Epoch 13/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 2.9195\n",
            "Epoch 13: loss improved from 3.18988 to 2.91948, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 2.9195\n",
            "Epoch 14/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 2.6624\n",
            "Epoch 14: loss improved from 2.91948 to 2.66239, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 2.6624\n",
            "Epoch 15/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 2.4173\n",
            "Epoch 15: loss improved from 2.66239 to 2.41734, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 2.4173\n",
            "Epoch 16/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 2.1799\n",
            "Epoch 16: loss improved from 2.41734 to 2.17989, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 2.1799\n",
            "Epoch 17/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.9454\n",
            "Epoch 17: loss improved from 2.17989 to 1.94537, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 1.9454\n",
            "Epoch 18/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.7317\n",
            "Epoch 18: loss improved from 1.94537 to 1.73174, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 1.7317\n",
            "Epoch 19/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.5240\n",
            "Epoch 19: loss improved from 1.73174 to 1.52396, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 1.5240\n",
            "Epoch 20/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.3398\n",
            "Epoch 20: loss improved from 1.52396 to 1.33979, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 1.3398\n",
            "Epoch 21/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.1876\n",
            "Epoch 21: loss improved from 1.33979 to 1.18764, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 1.1876\n",
            "Epoch 22/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 1.0531\n",
            "Epoch 22: loss improved from 1.18764 to 1.05310, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 1.0531\n",
            "Epoch 23/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.9470\n",
            "Epoch 23: loss improved from 1.05310 to 0.94705, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.9470\n",
            "Epoch 24/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.8633\n",
            "Epoch 24: loss improved from 0.94705 to 0.86333, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.8633\n",
            "Epoch 25/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.7942\n",
            "Epoch 25: loss improved from 0.86333 to 0.79423, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 0.7942\n",
            "Epoch 26/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.7403\n",
            "Epoch 26: loss improved from 0.79423 to 0.74034, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 118s 90ms/step - loss: 0.7403\n",
            "Epoch 27/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.7025\n",
            "Epoch 27: loss improved from 0.74034 to 0.70252, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 0.7025\n",
            "Epoch 28/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.6663\n",
            "Epoch 28: loss improved from 0.70252 to 0.66632, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.6663\n",
            "Epoch 29/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.6393\n",
            "Epoch 29: loss improved from 0.66632 to 0.63925, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.6393\n",
            "Epoch 30/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.6140\n",
            "Epoch 30: loss improved from 0.63925 to 0.61400, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 0.6140\n",
            "Epoch 31/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5933\n",
            "Epoch 31: loss improved from 0.61400 to 0.59326, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 0.5933\n",
            "Epoch 32/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5765\n",
            "Epoch 32: loss improved from 0.59326 to 0.57645, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.5765\n",
            "Epoch 33/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5565\n",
            "Epoch 33: loss improved from 0.57645 to 0.55652, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.5565\n",
            "Epoch 34/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5447\n",
            "Epoch 34: loss improved from 0.55652 to 0.54473, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.5447\n",
            "Epoch 35/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5351\n",
            "Epoch 35: loss improved from 0.54473 to 0.53510, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 88ms/step - loss: 0.5351\n",
            "Epoch 36/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5207\n",
            "Epoch 36: loss improved from 0.53510 to 0.52069, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.5207\n",
            "Epoch 37/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5075\n",
            "Epoch 37: loss improved from 0.52069 to 0.50749, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.5075\n",
            "Epoch 38/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.5016\n",
            "Epoch 38: loss improved from 0.50749 to 0.50158, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 88ms/step - loss: 0.5016\n",
            "Epoch 39/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4884\n",
            "Epoch 39: loss improved from 0.50158 to 0.48842, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4884\n",
            "Epoch 40/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4806\n",
            "Epoch 40: loss improved from 0.48842 to 0.48063, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4806\n",
            "Epoch 41/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4729\n",
            "Epoch 41: loss improved from 0.48063 to 0.47288, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4729\n",
            "Epoch 42/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4676\n",
            "Epoch 42: loss improved from 0.47288 to 0.46764, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4676\n",
            "Epoch 43/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4588\n",
            "Epoch 43: loss improved from 0.46764 to 0.45882, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4588\n",
            "Epoch 44/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4506\n",
            "Epoch 44: loss improved from 0.45882 to 0.45056, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4506\n",
            "Epoch 45/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4452\n",
            "Epoch 45: loss improved from 0.45056 to 0.44519, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4452\n",
            "Epoch 46/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4395\n",
            "Epoch 46: loss improved from 0.44519 to 0.43945, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4395\n",
            "Epoch 47/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4359\n",
            "Epoch 47: loss improved from 0.43945 to 0.43586, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4359\n",
            "Epoch 48/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4310\n",
            "Epoch 48: loss improved from 0.43586 to 0.43098, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 88ms/step - loss: 0.4310\n",
            "Epoch 49/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4261\n",
            "Epoch 49: loss improved from 0.43098 to 0.42614, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.4261\n",
            "Epoch 50/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4184\n",
            "Epoch 50: loss improved from 0.42614 to 0.41841, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 88ms/step - loss: 0.4184\n",
            "Epoch 51/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4137\n",
            "Epoch 51: loss improved from 0.41841 to 0.41367, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.4137\n",
            "Epoch 52/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4113\n",
            "Epoch 52: loss improved from 0.41367 to 0.41131, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.4113\n",
            "Epoch 53/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4060\n",
            "Epoch 53: loss improved from 0.41131 to 0.40599, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.4060\n",
            "Epoch 54/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4038\n",
            "Epoch 54: loss improved from 0.40599 to 0.40382, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.4038\n",
            "Epoch 55/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.4007\n",
            "Epoch 55: loss improved from 0.40382 to 0.40071, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.4007\n",
            "Epoch 56/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3981\n",
            "Epoch 56: loss improved from 0.40071 to 0.39806, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.3981\n",
            "Epoch 57/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3945\n",
            "Epoch 57: loss improved from 0.39806 to 0.39448, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.3945\n",
            "Epoch 58/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3903\n",
            "Epoch 58: loss improved from 0.39448 to 0.39035, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.3903\n",
            "Epoch 59/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3845\n",
            "Epoch 59: loss improved from 0.39035 to 0.38450, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.3845\n",
            "Epoch 60/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3830\n",
            "Epoch 60: loss improved from 0.38450 to 0.38295, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.3830\n",
            "Epoch 61/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3779\n",
            "Epoch 61: loss improved from 0.38295 to 0.37787, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 89ms/step - loss: 0.3779\n",
            "Epoch 62/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3726\n",
            "Epoch 62: loss improved from 0.37787 to 0.37263, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.3726\n",
            "Epoch 63/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3761\n",
            "Epoch 63: loss did not improve from 0.37263\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.3761\n",
            "Epoch 64/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3750\n",
            "Epoch 64: loss did not improve from 0.37263\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.3750\n",
            "Epoch 65/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3690\n",
            "Epoch 65: loss improved from 0.37263 to 0.36899, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 116s 89ms/step - loss: 0.3690\n",
            "Epoch 66/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3672\n",
            "Epoch 66: loss improved from 0.36899 to 0.36723, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 117s 90ms/step - loss: 0.3672\n",
            "Epoch 67/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3674\n",
            "Epoch 67: loss did not improve from 0.36723\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.3674\n",
            "Epoch 68/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3619\n",
            "Epoch 68: loss improved from 0.36723 to 0.36188, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.3619\n",
            "Epoch 69/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3624\n",
            "Epoch 69: loss did not improve from 0.36188\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.3624\n",
            "Epoch 70/70\n",
            "1308/1308 [==============================] - ETA: 0s - loss: 0.3565\n",
            "Epoch 70: loss improved from 0.36188 to 0.35645, saving model to next_words.h5\n",
            "1308/1308 [==============================] - 115s 88ms/step - loss: 0.3565\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e77603f43d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, num_words=10):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    predicted_words = []\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        padded_sequence = np.array(sequence[-3:]).reshape(1, -1)  # Adjust depending on your input length\n",
        "        preds = model.predict(padded_sequence)\n",
        "        pred_word_index = np.argmax(preds, axis=-1)[0]\n",
        "        predicted_word = tokenizer.index_word[pred_word_index] if pred_word_index in tokenizer.index_word else ''\n",
        "\n",
        "        predicted_words.append(predicted_word)\n",
        "        sequence.append(pred_word_index)  # Update sequence with the predicted index\n",
        "\n",
        "    return ' '.join(predicted_words)\n",
        "\n",
        "# Continuous prediction loop\n",
        "while True:\n",
        "    text_input = input(\"Enter your line: \")\n",
        "    if text_input == \"0\":\n",
        "        print(\"Execution completed.....\")\n",
        "        break\n",
        "    else:\n",
        "        try:\n",
        "            print(predict_next_words(model, tokenizer, text_input, num_words=10))  # You can change num_words as needed\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred: \", e)\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "W3fDar2XSssF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aefad72-1e68-41ca-9772-350e212a0b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: what are agents\n",
            "1/1 [==============================] - 1s 647ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "with certainty utility provides a way in which the agent\n",
            "Enter your line: what is artifical intelligence\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "3 turing’s brain in the 1990s including the international workshop\n",
            "Enter your line: what's the title of chapter 12\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "we show that any rational agent must behave as if\n"
          ]
        }
      ]
    }
  ]
}